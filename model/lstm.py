# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxuhAWvanfPUMvlm5cJ8iGy3jBQo1Y77
"""

!git clone https://github.com/kairosfuture/dcipher-nlp-challenge.git
!pip install pycodestyle
!pip install --index-url https://test.pypi.org/simple/ nbpep8

import json
import pandas as pd
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import spacy
from torchtext.data import Field, TabularDataset, BucketIterator
from sklearn.model_selection import train_test_split
import os
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from matplotlib import pyplot
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

from google.colab import drive
drive.mount('/content/drive')

def normalise_text(text):

    text = text.str.lower()
    text = text.str.replace(r"\#", "")
    text = text.str.replace(r"http\S+", "URL")
    text = text.str.replace(r"@", "")
    text = text.str.replace(r"[^A-Za-z0-9()!?\'\`\"]", " ")
    text = text.str.replace("\s{2,}", " ")

    return text

file = '/content/dcipher-nlp-challenge/data/wos2class.json'
with open(file) as df:
    dict_train = json.load(df)
df = pd.DataFrame.from_dict(dict_train)

df.isna().sum()
df.drop(df[df['Abstract'].str.len() < 10].index, inplace=True)
df['title_abstract'] = df['Title'] + ". " + df['Abstract']
df['Label'] = (df['Label'] == 'Material Science').astype('int')
df["title_abstract"] = normalise_text(df["title_abstract"])

train, test, y_train, y_test = train_test_split(df["title_abstract"],
                                                df["Label"],
                                                test_size=0.2,
                                                random_state=42,
                                                stratify=df["Label"])

train, val, y_train, y_val = train_test_split(train,
                                              y_train,
                                              test_size=0.2,
                                              random_state=42,
                                              stratify=y_train)

df_train = pd.concat([train, y_train], ignore_index=True, sort=False, axis=1)
df_val = pd.concat([val, y_val], ignore_index=True, sort=False, axis=1)
df_test = pd.concat([test, y_test], ignore_index=True, sort=False, axis=1)

df_train.to_csv('train.csv', index=False)
df_val.to_csv('valid.csv', index=False)
df_test.to_csv('test.csv', index=False)
df_train.to_json('wos2class.train.json')
df_val.to_json('wos2class.valid.json')
df_test.to_json('wos2class.test.json')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.deterministic = True  
seed = 17
torch.manual_seed(seed)

label_field = Field(sequential=False,
                    use_vocab=False,
                    dtype=torch.float,
                    batch_first=True)

text_field = Field(sequential=True,
                   tokenize='spacy',
                   batch_first=True,
                   include_lengths=True,
                   lower=True)

fields = {'1': ('label', label_field), '0': ('title_abstract', text_field)}

train, valid, test = TabularDataset.splits(path='./',
                                           train='train.csv',
                                           validation='valid.csv',
                                           test='test.csv',
                                           format='CSV', fields=fields)

text_field.build_vocab(train, min_freq=8, vectors="glove.6B.100d")

train_iter = BucketIterator(train,
                            batch_size=32,
                            sort_key=lambda x: len(x.title_abstract),
                            sort_within_batch=True,
                            device=device)

valid_iter = BucketIterator(valid,
                            batch_size=32,
                            sort_key=lambda x: len(x.title_abstract),
                            sort_within_batch=True,
                            device=device)

test_iter = BucketIterator(test,
                           batch_size=32,
                           sort_key=lambda x: len(x.title_abstract),
                           sort_within_batch=True,
                           device=device)

class lstm(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,
                 num_layers, bidirection, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.dropout = nn.Dropout(dropout)

        self.embeddings = nn.Embedding(vocab_size,
                                       embedding_dim,
                                       padding_idx=0)

        self.lstm = nn.LSTM(embedding_dim,
                            hidden_dim,
                            batch_first=True,
                            num_layers=num_layers,
                            bidirectional=bidirection)

        self.linear = nn.Linear(hidden_dim * 2, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, length):
        x = self.embeddings(input)
        x = self.dropout(x)

        x_pack = nn.utils.rnn.pack_padded_sequence(x,
                                      length.cpu(),
                                      batch_first=True,
                                      enforce_sorted=False)

        out_pack, (ht, ct) = self.lstm(x_pack)
        ht = torch.cat((ht[-2, :, :], ht[-1, :, :]), dim=1)
        dense_outputs = self.linear(ht)
        outputs = self.softmax(dense_outputs)
        return outputs

size_of_vocab = len(text_field.vocab)
embedding_dim = 100
num_hidden_nodes = 32
num_output_nodes = 2
num_layers = 2
bidirection = True
dropout = 0.2

model = lstm(size_of_vocab,
             embedding_dim,
             num_hidden_nodes,
             num_output_nodes,
             num_layers,
             bidirection,
             dropout)

pretrained_embeddings = text_field.vocab.vectors
model.embeddings.weight.data.copy_(pretrained_embeddings)

optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion  = nn.NLLLoss()

model = model.to(device)
criterion = criterion.to(device)

def train(model, iterator, optimizer, criterion):

    epoch_loss = 0
    epoch_acc = 0
    model.train()

    for batch in iterator:

        optimizer.zero_grad()

        title_abstract, title_abstract_lengths = batch.title_abstract
        labels = batch.label
        preds = model(title_abstract, title_abstract_lengths)
        loss = criterion(preds, labels.long())

        preds = torch.argmax(preds, axis=1)
        correct = (preds == labels).float()
        acc = correct.sum() / len(correct)

        loss.backward()

        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion):

    epoch_loss = 0
    epoch_acc = 0
    model.eval()

    with torch.no_grad():

        for batch in iterator:

            title_abstract, title_abstract_lengths = batch.title_abstract
            labels = batch.label
            preds = model(title_abstract, title_abstract_lengths)
            loss = criterion(preds, labels.long())

            preds = torch.argmax(preds, axis=1)
            correct = (preds == labels).float()
            acc = correct.sum() / len(correct)

            epoch_loss += loss.item()
            epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)

from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

N_EPOCHS = 25
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    train_loss, train_acc = train(model, train_iter, optimizer, criterion)

    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), '/content/drive/MyDrive/weights_dcipher/lstm_weights.pt')

    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

with torch.no_grad():
    torch.cuda.empty_cache()
path = '/content/drive/MyDrive/weights_dcipher/lstm_weights.pt'
model.load_state_dict(torch.load(path))

model = model.to(device)

preds = []
probas_0 = []
probas_1 = []

for i, batch in enumerate(test_iter):

    title_abstract, title_abstract_lengths = batch.title_abstract

    with torch.no_grad():
        pred = model(title_abstract, title_abstract_lengths).float()
        probas_0.extend(pred[: , 0:1].squeeze().detach().cpu().numpy())
        probas_1.extend(pred[: , 1:2].squeeze().detach().cpu().numpy())
        pred = torch.argmax(pred, axis=1)
        pred = pred.squeeze().detach().cpu().numpy()
        preds.extend(pred)

preds = np.asarray(preds)
print(classification_report(y_test, preds))
pd.crosstab(y_test, preds)

auc_1 = roc_auc_score(y_test, probas_1)

print('Class 1: ROC AUC = %.3f' % (auc_1))

f1 = f1_score(y_test, preds)
print('F1 Score = %.3f' % (f1))

acc = accuracy_score(y_test, preds)
print('Accuracy = %.3f' % (acc))

ns_1 = [1 for _ in range(len(y_test))]

fpr_0, tpr_0, _ = roc_curve(y_test, probas_0)
fpr_1, tpr_1, _ = roc_curve(y_test, probas_1)
fpr_ns_1, tpr_ns_1, _ =  roc_curve(y_test, ns_1)

pyplot.plot(fpr_ns_1, tpr_ns_1, linestyle='--', label='Only 1')
pyplot.plot(fpr_1, tpr_1, marker='.', label='Model')
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
pyplot.show()

